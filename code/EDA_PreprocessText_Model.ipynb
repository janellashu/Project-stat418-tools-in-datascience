{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='ba5Zmyom6UYv1Q', \n",
    "                     client_secret='G0osnze26nmm8mcARteuQ9Du748',\n",
    "                     user_agent='Python research tool by Janella Shu, janshu@g.ucla.edu',\n",
    "                     username= '?????',\n",
    "                     password= '?????')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######getting information about submission made by russian accounts#######\n",
    "\n",
    "#using PRAW\n",
    "authors = ['BlackToLive', 'FaurnFlamebreaker', 'bill_jonson', 'PurebringerOghmagra', 'clawisma', 'jake_browny', 'Georgie_Felix', 'george_BENTLEY', 'rico_penya', 'Leprechaun_yura', 'Admiraf', 'lgbtunited', '20twony', 'momaskin', 'FelhafymDugrel', 'DoriraKam', 'DalaghmaMajurus', 'IfiacelMoogutaxe', 'AragelvBlackkiller', 'SsoncenModifyn', 'KulagdaMneriel', 'BloodbearerBusius', 'SteeldefenderZululm', 'TaushakarGrantrius', 'FlameweaverTokazahn', 'MunirusDrelalas', 'JoJosarBlacksong', 'ZulkilkreeMazucage', 'RexcliffVushura', 'KizragoreMajin', 'MograMarn', 'RedseekerMami', 'StardwellerFordrezi', 'PaineyeDaron', 'RexscarNalmekelv', 'SaberstalkerTygorr', 'GhoginnModilbine', 'BeariArashura', 'ZalarGroshakar', 'DaktilarMarirus', 'GhozarIannn', 'AdofynMorakus', 'FelorielBalladolore', 'SaimathJurisar', 'ZulkizragoreLairdan', 'BrargasYggndis', 'FordreninDalas', 'GavidiKenrad', 'NikojindDalarad', 'SharpgroveMagal', 'AkinotilarGardana', 'ShadowhammerFelozan', 'FelorgasAlsagelv', 'ArashillCererana', 'YozshulkisNighthamm', 'FelothisRagegrove', 'KigagrelMightrunner', 'BalbineSnowbourne', 'HugilmeenaBurinrad', 'BagraBludwood', 'TetaurDizragore', 'KazshuraDoommaster', 'KularimGaviril', 'TaulabarMumi', 'NilkisZargas', 'DoukasaZahelm', 'BalladoranaBalhala', 'IshnntriusBufym', 'ManadorSternfont', 'SaithrisShakajas', 'ArarisarStarbinder', 'MakazahnFelodwyn', 'KulaNegore', 'ShalirgasFetaur', 'PeghmaGoldenfist', 'DawnravenFenritaur', 'HugirisBoth', 'DilarNitius', 'NelrajasAndromardre', 'DoukusGujar', 'TaushuraJujind', 'MavehelmMightblade', 'TanrisVijinn', 'MolarZulugar', 'KajizragoreJojurus', 'KulariTezshura', 'MorlurusShagelv', 'GaranBurithris', 'JurgKagashakar', 'MalarilZolorn', 'KenisLightwing', 'DokelvCentrillador', 'VolkisDulmaran', 'ThobandisGavinaya', 'GonosShak', 'ModidredAnanadar', 'RainwormKigak', 'DoukSaris', 'ThorisPerintrius', 'MazukazahnFlamebrin', 'AdriesidaVolmaran', 'ThotiusBalis', 'KigashoBlackmoon', 'YorgAndrozan', 'AndromalladorAnayan', 'MorardanaForad', 'GrojoraAxebinder', 'RunebringerSaberdra', 'GavirusBurus', 'GavizelMazumi', 'MajinnSpellgrove', 'RaindwellerKasho', 'NafymAriugelv', 'ShalilisDaill', 'vadro', 'jay_cook', 'eblun', 'marshallwayneee', 'thomas_baxter', 'chrisrockyou', 'tomwiek', 'IndianaLewis666', 'peterpenpeter', 'AllanMcPherson2', 'GeraldineKnapp', 'coolpau', 'RichardPhilbrick', 'MichelleBerends', 'm_peltier', 'hymynameisharold', 'chillypepperchilly', 'marcus_jones', 'lisa_kelley', 'lipstickred', 'sasanbasan', 'sunshinesunny', 'RichardShannon', 'nu3goxyu', 'TaraLynnRussel', 'YnellieryFezilkree', 'MeztimGavijurus', 'DathrisKelabar', 'MeztikusVusar', 'DoularZulkijas', 'KariusToshura', 'KaziraSirardin', 'DarimTatilar', 'FelordanaAkinojinn', 'KazitaurTruemane', 'BlackforgeKazikazah', 'AdorahuginnKele', 'BallariusAgamantriu', 'AdorakelvFelolis', 'GholbinradCentrilsa', 'KazilCentrilhala', 'DurnAdrierdana', 'FrostmoonVur', 'LandaathDalbine', 'TygraswynBloodblade', 'TruthwindDawnhunter', 'CordalenKeralsa', 'FenriranWhiteeye', 'MarinaraMolis', 'FaucageDoomsmith', 'MalakusShagore', 'DagdagrinnThordin', 'MezizilkreeRunewind', 'SaithilarOghmarus', 'MoramarVorr', 'AralmaranGajinn', 'ShajarDalardred', 'BranayaKeletius', 'BuMolhala', 'SaithiniusManakus', 'ThunderbringerKagre', 'AratGavith', 'RedhammerVusho', 'FernRunecaster', 'DofymZumuro', 'DirehammerDoomworm', 'BuzarusNathris', 'DagdalarDoulrajas', 'BeazedredLaighma', 'ShalrajasMightbring', 'BuzaradAna', 'TaularLightkiller', 'FenrisMnegrinn', 'HellbearerIshn', 'NajasMecage', 'DokasaDoushakar', 'BalaceDait', 'AnararanaModith', 'DougoreDaghma', 'GholbiziusGramand', 'CenradNuarad', 'MubandisAxeweaver', 'BuriathFelordred', 'GholisMazumi', 'FearlessbrewGazrago', 'DawnsongMalador', 'DagdalenShaligra', 'TygolkreeTygorn', 'BlackstalkerZulkisi', 'GorgMoogukus', 'MorlugdaMugore', 'KagalkisArigar', 'CemeenaShaswyn', 'ZujarDalara', 'OpiwieldKirr', 'AdriekusFlameworker', 'TholoreMorlurgas', 'ModiborThorgalis', 'KazrajoraMofyn', 'VujoraNightworker', 'MalogamandGrarne', 'WrathshadeButh', 'ZulkibeiTruefire', 'DaitIshnth', 'LasiusNalmeswyn', 'NagelvConjugar', 'KajimShadoweye', 'JoJotTygramand', 'UmlasMekus', 'IanradKisho', 'KanosDoulrajas', 'TaujinnStonewind', 'DozuruSteelbringer', 'KathribandisZa', 'ThunderbringerKajin', 'MoonshadeGavifyn', 'GranithisKirizar', 'TetAndromalak', 'PerilasBlackflame', 'BroadshaperNualar', 'FordrenisBloodstaff', 'CentrilvKashura', 'RockravenShaktilmar', 'ColasDushura', 'MunilsaNilanin', 'DianatiusFaern', 'GoshuraAdrienn', 'FenrilmaranMarin', 'TygrargasShadowstaf', 'CedredKirimath', 'ZaDacage', 'GravelbeardUmdar', 'DutBlackshaper', 'DuraKiririm', 'NikolabarMezticage', 'FordregdaNualis', 'FordremeenaModred', 'TaulrajasHugifyn', 'FelhazarVim', 'KathridarSaigar', 'BeazedredHunin', 'GuzuruKarin', 'KatiusNalmeginn', 'FomandAndroath', 'KazikazahnShakalkis', 'MavegrinnOghmagar', 'YozshurnDousar', 'MalanimSashakar', 'KazilkreeGavidi', 'ThetatiusTojatilar', 'AndrothrisAkile', 'ZulkilmaranFearless', 'AnayarnDusar', 'AuziusPebor', 'LighthammerAdora', 'OghmarielMorarne', 'KeralvMalarin', 'BeazekusGavilmeena', 'KekoraFejar', 'FaeshuraDirehammer', 'FoBeazezar', 'KaborBearis', 'GarisarCentri', 'BanrisShagal', 'ZulkizuruMauzahn', 'DelameenaDailar', 'CordaradKeraswyn', 'MenrisLainn', 'KejindGolabar', 'MalakusGobei', 'WhispermaneGakinos', 'ToktilarJular', 'ShaathRedweaver', 'GullMalazuru', 'TygozilkreeDandis', 'SamuhnCelen', 'BraathGhondis', 'DarkshaperWhiteblad', 'DozahnBlacksmasher', 'MalarnBuginn', 'NuadagasMudor', 'FeloraShadowbringer', 'MaloathFordrethis', 'BandidarKagaramar', 'GoltigoreArashishak', 'FolasMogrel', 'GamiBalladolune', 'AurilakSaijurus', 'DainosAndromaswyn', 'BlackstaffCordazar', 'NikinosTygranius', 'NiramarSaithithis', 'GriborSasho', 'MiranadarAnardana', 'UmluneYolar', 'VozilkreeStonebreak', 'MagedwellerMaloriel', 'BusidaHuginadar', 'DugisSabermaster', 'BuVoodoogul', 'AgathrisRockweaver', 'DuzshuraMorlurn', 'StarsmithPurefang', 'PezarMadal', 'GholbisiusAdrieginn', 'PerilasMawield', 'NaraWhitehunter', 'PeridwynSaithilis', 'FauraBugas', 'AnayaronAdorarin', 'MekreeKeswyn', 'BlackraySinshaper', 'DabeiSharpdefender', 'GoldskinKelmaran', 'KajihnGavinn', 'SindwellerKerarne', 'AgafynFefym', 'BallariFelorus', 'MoranShadowdragon', 'DalasMorlubandis', 'RunebourneMavetus', 'JothisMokus', 'MezilrajasRainconju', 'DawnfistKirirdana', 'JutAkinolar', 'MakinosMazugis', 'DoginnSirarim', 'NalkreeKadwyn', 'LajurusOghmagamand', 'BearadGakus', 'PerilisMezisida', 'FordrehuginnNilarga', 'GoltirrNuadaswyn', 'SaberwoodMikakazahn', 'TernCoidar', 'IanrisFordrebor', 'KelePerdin', 'BallargasFenribar', 'ArijurusRunesmasher', 'SaitiusGrilen', 'BurilNuadadred', 'MoogumuroGalrajas', 'NalmeathFordreril', 'SaithihelmDathris', 'DoomseekerDakelv', 'MoogukoraDugore', 'BuzathisWhitebrew', 'NiNizahn', 'OpindisCordafyn', 'MutaxeDorinrad', 'MalatiusJokus', 'MalmaranBurilbine', 'NaziusTujora', 'ShalilsaVor', 'CererneGardasar', 'AriloreKesida', 'ConjularAdolore', 'TaukoraBrakus', 'ThordanaLaigelv', 'MavergasTholak', 'SardredWhiteblade', 'SiramandThordirgas', 'NalisShaktimi', 'KazraktilarBluebear', 'ShaktisidaMoogum', 'SteelbrewGavirad', 'MolarMavellador', 'AriginnKerin', 'GranadarPenrad', 'BuzalisFern', 'DofymArcaneredeemer', 'PezanArigrinn', 'NuadagarKelenadar', 'KigashicageTegas', 'ThunderwalkerAurida', 'SanadarDagdara', 'ShalirusDoomcliff', 'AuririBeagrinn', 'BuzagasVinris', 'AdoswynShakam', 'YotilarMaramar', 'SamukazahnThunderfu', 'AkinorgWhisperforge', 'GraveleyeGardana', 'BakTygralore', 'ShalimathKekelv', 'KajoraDelalas', 'KejindMoogucage', 'ArashiraMirantrius', 'ThordibandisCentrin', 'IronhammerGholbithr', 'BoathDoushakar', 'DianarneFelhandis', 'DianafymMeztira', 'UmgraDoomcaster', 'GralladorTojall', 'SinbinderMezikasa', 'ThunderbreakerTuram', 'IshnnayaKendis', 'MorinBandikelv', 'ZurFergas', 'BragulBuzagda', 'ManarinDorizar', 'NarrTauzuru', 'TygotDorifyn', 'MadiIanlace', 'BuzariusCowield', 'BeazarStoneflame', 'WhitebrewOghmalore', 'MarinradMorlundis', 'DalsaGholbilis', 'ShalinadarForcegrov', 'MagulZolodal', 'JuzshuraDusho', 'AnagasAdorann', 'SatiusKathrilar', 'MolabarThetalis', 'KeleginnGa', 'MenosMerg', 'MaurisarKiri', 'IanniusFaurr', 'KelelarArindis', 'MebeiManadwyn', 'BlackbreakerDalalsa', 'BaloreDarne', 'NakazahnDogul', 'BurikusVogar', 'DogarSaithigamand', 'TruesingerYggbandis', 'GoltijinPurepick', 'DoridorMalanrad', 'VishoSardred', 'BlackfangSinbeard', 'GujarBuzarius', 'BalabarTruesinger', 'BroadsongKegar', 'SamularMavelas', 'AnayawynIansida', 'YozshulkreeKathriwi', 'DiananradKula', 'ButhrisNecage', 'GrosarGhollador', 'DirehammerTojajin', 'GranradAgazar', 'ZagelvKirilmeena', 'MebarConjulhala', 'DadredGawield', 'ThohuginnDianardred', 'BluerayAkinorn', 'NirdinKazrall', 'KulanadarMazubar', 'NightravenJoJotilar', 'GutilarBlackraven', 'GhosiusAraswyn', 'NitFothris', 'MirandisStoneblade', 'FrostdragonTukus', 'FlamebringerRockdwe', 'GaluneManis', 'ModinadarKamuro', 'KigaleLaidred', 'KigajoraTruemaster', 'KedarGavirne', 'PeriBeamath', 'WindscarThetarim', 'MalazelHellwarden', 'FlamesmasherMalordr', 'ThunderconjurorDark', 'DaranBlueworker', 'KerameenaShalimath', 'AnarardredStonebran', 'FrostshadeMozil', 'ArashirAgarin', 'ThorgalarKalar', 'KargBeazeswyn', 'OghmalarDoomray', 'ThontriusMaulkis', 'GornSham', 'NikomuroCeris', 'IronwormGrirdana', 'VojinThordithis', 'GamathTura', 'SpellhunterGaktilar', 'MneborFlamemaster', 'SilverdefenderNuada', 'MalasarCordanius', 'AndromaziusShakalar', 'YggwieldKagakree', 'YozshusidaMujora', 'MuthYole', 'KagazuruMarin', 'JumuroFlameweaver', 'MajindSirahuginn', 'HuginnShantrius', 'NightbreakerCefym', 'IroneyeManagar', 'NikokazahnSpellston', 'DarkbreakerBeazenar', 'RagescarTejin', 'StarredeemerFlamebr', 'AuriborSternworker', 'ConjuDrelatius', 'DagdalisStoneweaver', 'MoonwardenAdrierdin', 'KigagorZujin', 'DailarKezshura', 'DianatusSharpcliff', 'SaberwardenBabandis', 'BalladothisBragami', 'KillTygoll', 'MaveghmaSamurg', 'MosidaBlacksinger', 'BagraMolar', 'TegrelKajilkree', 'KillThowield', 'AdorariusBrightflam', 'SwordfistBraktilar', 'KaziktilarYokus', 'MirnSairne', 'MaririelNila', 'KeramarKemand', 'ArashigrelVudolmara', 'LamathConjurdin', 'GozshuraTagrel', 'MuzilDoritius', 'CewieldGranius', 'CordaKat', 'MoshicageCentrirdre', 'TauzragoreAndrolune', 'AndromajurusDianage', 'FlamebinderFelhadre', 'GoldenwoodAnayawiel', 'PainforgeKigagis', 'DoutaxeDijora', 'FelhardredLandabor', 'KagataxeDala', 'OpintriusMalador', 'GoltirnAdrie', 'GraniziusAndromaghm', 'VuzilHuwyn', 'MunirielDawnsmith', 'MadiTor', 'MashuraManalmeena', 'KakelvGror', 'DarisFelar', 'TetaurTashura', 'BroadbrandStoneweav', 'KirnGavinrasida', 'ArisidaFaekasa', 'BlackwardenVudozil', 'AurinisNuadardred', 'GardalmaranMelkis', 'AdoralanimBlackpick', 'KazratBeazenn', 'TygralasModidred', 'BuriAnage', 'MoogugorAkinotaxe', 'GriraAgabor', 'NiladredMajar', 'GrinayaMunilanim', 'BumeenaJulrajas', 'MumiMikagis', 'BeazegdaAgaril', 'BloodshadeMoonweave', 'JusarCoginn', 'MetilarMavenadar', 'DogisVogrel', 'NightwindLagar', 'KazragoreMale', 'AnaraninMagar', 'DagdaranaAkilmaran', 'ShakalrajasTholsa', 'WhitebringerTewyn', 'KagazshuraDoulrajas', 'KagorJuhn', 'AuridredDarad', 'VisarColace', 'RainsongTojarn', 'SteelfontKeleris', 'ZukazahnAurifym', 'LandadorJoJora', 'StonewoodCoirus', 'SiralhalaYojin', 'SinweaverMordred', 'AurigeAnarin', 'KiriTygramath', 'ArajinTaukazahn', 'MalarAdosius', 'ShaktilkisTumuro', 'YorgConjunara', 'JoJolarKathrimand', 'paulgrapes', 'rapitangnyy', 'artautumn', 'dipperhummer', 'Ten_GOP', 'ny_black_protest', 'andy1235', 'magaing4trump', 'gogomrbrown', 'MatEvidence', 'lilstoner', 'jenn_abrams', 'leecory', 'georgeschultz', 'KularimGaviril', 'miajones005', 'rblackie', 'hollymolly2015', 'lydia_rodarte-quayle', 'Motlalepule', 'Motlalent', 'Bugemlon', 'Amohelang', 'Eliot_Aiden', 'Tyrone_Marion', 'Jaden_Corey', 'Aiden_Chuck', 'Tristen_Sadler', 'LorrySn', 'deren777', 'sarah_sanchez', 'julesbrandy', 'HelenaFreiy', 'smarteagles', 'geoffreych', 'MarkHudson89', 'FuUNCake', 'tebepizdec', 'rebekahbennet', 'hcaner', 'bilberrypie', 'vsbeats', 'fergensonk', 'illlolyourtexass', 'astridrooo', 'lunacyfreak', 'cre-ker', 'valyaspb', 'joewellbenson', 'ivanlox255', 'huyolzovatel', 'pizdectest', 'kozloshlyap', 'picnicshirt', 'issastunna', 'stendupp', 'stenlies', 'failkate', 'smoliangirl', 'mironon', 'traceyjohnson', 'stephenbag', 'bennylewis', 'susanpeterson', 'madpood', 'amazingbodilyfluids', 'qkempek', 'sandlerwow', 'BleepThePolice', 'merlinperegrin', 'Albert_Baldwin', 'Tayler_Cletis', 'vsrruslan', 'ser005', 'YggkelvZurn', 'KazishuraTerne', 'baconlover8', 'SouthLoneStar', 'Lepolesa', 'moluleloa', 'YggwynZulkigul', 'vsruslan', 'Kutloisiso', 'Hararnir', 'georgepharell', 'kuabrush', 'mikeehrmantraut55', 'salamanca_tuco', 'WileandraMalhala', 'RobbKR', 'ThorgalenGozil', 'wolfen01', 'jessepinkman1984', 'letlotlow', 'solodoto666', 'saulgoodman1978', 'Mamigon', 'TakeMyEnerGG', 'ZuluraGolar', 'anime_stan', 'khotsofalan', 'Ezra_Jimmie', 'QuenceynnTejora', 'IronhammerConjukelv', 'trollsofficial', 'fring-gus', 'truckshark', 'FezilGavinn', 'AdoraronDoomworker', 'ZolosidaSpellsinger', 'KahuginnCerenadar', 'YokouJJ', 'VuranThordilore', 'Brenden_Kevyn', 'LandageKador', 'BrahnVilar', 'GaranaTobar', 'YggdorKakree', 'graulli', 'Heirlow', 'FordreranaCordagda', 'randomipad', 'SailakSamura', 'smollow', 'socketwom', 'foxmeowlder', 'CeriusTojajar', 'ModitiusOpirius', 'mohato', 'Abena_Tau', 'GranilisDogul', 'BloodwoodArashizrag', 'crownalbum', 'DaytonayenMoranaya', 'CewieldZuluhn', 'Taulou', 'VoodoogamiAurilak', 'HaroldGarold', 'OghmagelvFedora', 'davscolf', 'LightshadeGardakino', 'TetaurDafyn', 'HoodieLoodie', 'walterwhite1962', 'Derek_Daniel', 'KagrelMauran', 'NilarilOpirim', 'DrelathrisBo', 'SalmaranLaihuginn', 'mr_clampin', 'VanecedwaShadowsmith', 'SadCitizen2016', 'seatfaith', 'AgaluneMalordred', 'MeepoPeep', 'KakreeTabar', 'thistleescape', 'Garry_Gregg', 'WilliananLandalsa', 'germagamo', 'inkreindeer', 'TojasidaSamukree', 'hank-schrade', 'IshnranaAnayagrinn', 'aubrey_harper', 'ModitiusFaetaur', 'GushicageAgamadi', 'NuaborShaktikasa', 'garliccomet', 'jerrhill', 'moonreading', 'MalojurusMutus', 'Riley_Gerrard', 'AralisTugis', 'FreddieGi', 'CogelvWhitebinder', 'VojarAndrorus', 'SharprunnerAndrolas', 'winthrop_clancy', 'emilyli', 'BagoreZulkihn', 'muslimzebra', 'DorothieBell', 'Orville_Willy', 'wadeharriot', 'KatusPenadar', 'KulazelKelkree', '1488Reasons', 'TojatMalaron', 'dreamsquadd', 'DalabandisAndroma', 'cakes689', 'Zach_Bristol', 'riversailor', 'SteelfangMogamand', 'bottlehope', 'Gilbert_Preston', 'harperdaveharper', 'chriscrossfire', 'GavinayaBeazefym', 'Paolosi', 'SnorrySturluson', 'GoldenbringerTogore', 'NalenTaugami', 'MoonMoon8', 'Darcy_Loyd', 'DianarinAdorardred', 'ThorgaghmaLai', 'FerdavynaCentrirus', 'petouchoque', 'AriundisVugar', 'CordaronGavinralbin', 'DelaranaBatius', 'BallardredTholak', 'MalalladorSincliff', 'jojouel', 'violinslipper', 'caterpillarclass', 'AxeseekerMightwind', 'SamugoreKekelv', 'patsy_rudolph', 'DelalbineSilvershad', 'DearDearBear', 'DarusCege', 'RedGov', 'malefane', 'PeriniusDular', 'vudonrismakasa', 'VukazahnGardagis', 'AndromajurusAragrel', 'HityndiDutilar', 'NekoraJoJolar', 'Dazzle6', 'rockanda', 'IronwalkerGardred', 'DelanradAgamadwyn', 'FenrilkreeMolabar', 'BludflameMeztirg', 'researchbeggar', 'toffeetear', 'AndrorneGavinrage', 'TebenelBrazragore', 'DaFordremand', 'SharpwingChillshape', 'Gillian0zero0', 'CererinKarn', 'ZukoraTezil', 'Mponeng', 'MnefymPainmane', 'HellwindShadowbreak', 'richwilcom', 'moratutu', 'DalahelmFelharil', 'paulmall112', 'AgamagelvTozshura', 'tishcol', 'lekgotla', 'xameg', 'PeGobei', 'blackie_jack', 'BurigelvGravelbrew', 'MalasarGholbiris', 'UmthDarkfire', 'Graham_Quinn', 'brondran', 'alex_jonathan', 'sugarshoehorn', 'LeyleynFearlessworm', 'CeginnFonin', 'Heetmean', 'GoltijindGoltishura', 'horseeating', 'Meek_Mitchel', 'hadheart', 'men_like', 'ThordibandisHugimat', 'waeqxd', 'hyddrox', 'CogdaAnayanara', 'brant_s', 'ZamalynodeThothis', 'fginery', 'come-true', 'CereranaKeath', 'chereese', 'TojasHellwarden', 'toffeeathletics', 'laserathletics', 'peter_stevenson1986', 'GavinraraFonara', 'keklelkek', 'ThonisIshnlen', 'ironzion17', 'ThontriusBanos', 'NitaurMaull', 'MananaraGralsa', 'kanyebreeze', 'LalhalaGavinradwyn', 'NualvCordalace', 'gordon_br', 'KiririelCebandis', 'UelitheLandagelv', 'dandy1crown', 'GrisidaColak', 'alice_boginski', 'fungon', 'MiraranaMogra', 'uelithelandagelv', 'FoshantBloodstone', 'BeazerneMem', 'mandeyboy', 'AriutusMokazahn', 'clackie', 'reggaebull', 'AlsagelvBuriron', 'deusexmachina112', 'elsie_c', 'TedarYozshujin', 'toneporter', 'SinmoonYggbandis', 'dopplegun', 'Maineylops', 'Maxwel_Terry', 'DeusXYX', 'MasiusShadowshaper', 'MargasGranidor', 'Peter_Hurst', 'erivmalazilkree', 'King_Andersons', 'BerskyN', 'WhatImDoindHere', 'Kevin_Milner', 'shomyo', 'rubinjer']\n",
    "i = 0\n",
    "test=[]\n",
    "while i < 665:\n",
    "    first_author = reddit.redditor(authors[i])\n",
    "    submission_user = []\n",
    "    for submission in first_author.submissions.new(limit=None):\n",
    "        submission_add = submission.id\n",
    "        submission_add2 = submission.author\n",
    "        submission_add3 = submission.created_utc\n",
    "        submission_add4 = submission.is_self\n",
    "        submission_add5 = submission.name\n",
    "        submission_add6 = submission.selftext\n",
    "        submission_add7 = submission.title\n",
    "        submission_add8 = submission.url\n",
    "        submission_add9 = submission.subreddit.id\n",
    "        submission_add10 = submission.subreddit.name\n",
    "        submission_user.append([submission_add, submission_add2, submission_add3, submission_add4, submission_add5, submission_add6, submission_add7, submission_add8, submission_add9, submission_add10])\n",
    "    del submission_user[0]\n",
    "    if i % 2 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        pass\n",
    "    test.append(submission_user)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########unable to get submission from two users \n",
    "#user = reddit.redditor(authors[665]), response \"Forbidden: recieved 403 HTTP response\"\n",
    "#user = reddit.redditor(authors[666]), suspended\n",
    "if hasattr(user, 'fullname'):\n",
    "    pass\n",
    "elif hasattr(user, 'is_suspended'):\n",
    "    print(user, 'suspended')\n",
    "else:\n",
    "    print(user, 'shadowbanned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######getting the remaining users###########\n",
    "i = 667\n",
    "test2=[]\n",
    "while i < len(authors):\n",
    "    first_author = reddit.redditor(authors[i])\n",
    "    submission_user = []\n",
    "    for submission in first_author.submissions.new(limit=None):\n",
    "        submission_add = submission.id\n",
    "        submission_add2 = submission.author\n",
    "        submission_add3 = submission.created_utc\n",
    "        submission_add4 = submission.is_self\n",
    "        submission_add5 = submission.name\n",
    "        submission_add6 = submission.selftext\n",
    "        submission_add7 = submission.title\n",
    "        submission_add8 = submission.url\n",
    "        submission_add9 = submission.subreddit.id\n",
    "        submission_add10 = submission.subreddit.name\n",
    "        submission_user.append([submission_add, submission_add2, submission_add3, submission_add4, submission_add5, submission_add6, submission_add7, submission_add8, submission_add9, submission_add10])\n",
    "    del submission_user[0]\n",
    "    if i % 2 == 0:\n",
    "        print(i)\n",
    "    else:\n",
    "        pass\n",
    "    test2.append(submission_user)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######combinine the 2 lists of submissions together and getting a list of subreddits##########\n",
    "testall=[]\n",
    "testall.extend(test)\n",
    "testall.extend(test2)\n",
    "\n",
    "test3=[]\n",
    "i=0\n",
    "while i < len(testall):\n",
    "    if len(testall[i]) > 0:\n",
    "        j=0\n",
    "        while j < len(testall[i]):\n",
    "            test4 = testall[i][j][8]\n",
    "            test3.append(test4)\n",
    "            j += 1\n",
    "    else:\n",
    "        pass\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list): \n",
    "  \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "  \n",
    "    #for key, value in freq.items():\n",
    "    #    print (key+\":\"+str(value)) \n",
    "    return freq\n",
    "  \n",
    "##########counting the frequency of each subreddit############\n",
    "if __name__ == \"__main__\":  \n",
    "    my_list = test3\n",
    "    subreddit_freq = CountFrequency(my_list) \n",
    "    \n",
    "subreddit_freq2 = {k: v for k, v in sorted(subreddit_freq.items(), key=lambda x: x[1], reverse = True)}\n",
    "subreddit_freq3 = list(subreddit_freq2.keys())[0:10]\n",
    "subreddit_freq4 = list(subreddit_freq2.values())[0:10]\n",
    "\n",
    "subreddit_freq5 = list(reddit.info(['t5_2qh33',\n",
    " 't5_31cjv',\n",
    " 't5_2qqdb',\n",
    " 't5_2qm21',\n",
    " 't5_2qt55',\n",
    " 't5_38unr',\n",
    " 't5_2qh1o',\n",
    " 't5_2qh3l',\n",
    " 't5_2qhgd',\n",
    " 't5_2wlj3',]))\n",
    "\n",
    "test1 = pd.DataFrame(subreddit_freq3, columns = ['subreddit_id'])\n",
    "test1['subreddit_name'] = [str(sub) for sub in subreddit_freq5]\n",
    "test1['subreddit_count'] = [int(count) for count in subreddit_freq4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########plot top 10 subreddits ##################\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot('subreddit_name', 'subreddit_count', data=test1[['subreddit_name', 'subreddit_count']], marker='', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Get all submission ids from Bad_Cop_No_Donut for 2016 using Pushshift io ############\n",
    "def getPushshiftDatafirst(start):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit=Bad_Cop_No_Donut'+'&after='+str(start)+'&size=500'\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    test7 = pd.DataFrame.from_dict(data['data'])[['author','created_utc','title','id']]\n",
    "    test8 = test7.values.tolist()\n",
    "    return test8\n",
    "\n",
    "def getPushshiftData(start, end):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit=Bad_Cop_No_Donut'+'&after='+str(start)+'&before='+str(end)+'&size=500'\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    test7 = pd.DataFrame.from_dict(data['data'])[['author','created_utc','title','id']]\n",
    "    test8 = test7.values.tolist()\n",
    "    return test8\n",
    "\n",
    "def unique(list1): \n",
    "    unique_list = [] \n",
    "    for x in list1: \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "begin = 1451606400-1\n",
    "final = 1451606400+60*60*24*365+1\n",
    "\n",
    "date = begin\n",
    "data_test = []\n",
    "\n",
    "while date < final:\n",
    "    last_created_utc = str('none')\n",
    "    if date == begin:\n",
    "        data_test = getPushshiftDatafirst(date)\n",
    "        last_created_utc = data_test[-1][1]\n",
    "    elif date > begin and date < final:\n",
    "        data_test_add = getPushshiftDatafirst(date)\n",
    "        data_test.extend(data_test_add)\n",
    "        last_created_utc = data_test[-1][1]\n",
    "    #elif date == final or date > final:\n",
    "    #    data_test_add = getPushshiftData(date, final)\n",
    "    #    data_test.extend(data_test_add)\n",
    "    date = last_created_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test2 = unique(data_test)\n",
    "\n",
    "i=0\n",
    "counter = 0\n",
    "while i < len(data_test2):\n",
    "    if int(data_test2[i][1]) > final:\n",
    "            counter = counter + 1\n",
    "    else: \n",
    "        pass\n",
    "    i += 1\n",
    "\n",
    "data_test3 = data_test2[:len(data_test2) - counter]\n",
    "#print(len(data_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########a list of all submission ids ##########\n",
    "data_test3_df = pd.DataFrame(data_test2, columns = ['author', 'created_utc', 'title', 'id'])\n",
    "\n",
    "#today = pd.Timestamp('today')\n",
    "#'filename_{:%m%d%Y}.json'.format(today)\n",
    "\n",
    "data_test3_df.to_json(r'submissions05282019.json', orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###START HERE: get list of submission ids##\n",
    "data_final = pd.read_json(r'submissions05282019.json', orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final2 = []\n",
    "i = 0\n",
    "while i < len(data_final):\n",
    "    nextsub = list(data_final.iloc[i])\n",
    "    data_final2.append(nextsub)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########a list of all submission ids ##########\n",
    "i = 0\n",
    "subid = []\n",
    "while i < len(data_final2):\n",
    "    #nextsubid = ['t_5'+str(data_final2[i][1])]\n",
    "    nextsubid = [str(data_final2[i][2])]\n",
    "    subid.extend(nextsubid)\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######using list of submission ids in PRAW to get other variables########\n",
    "\n",
    "data_praw = []\n",
    "i = 0\n",
    "\n",
    "while i < len(data_final2): \n",
    "    submission = reddit.submission(id=str(subid[i]))\n",
    "    submission_add = submission.id\n",
    "    submission_add2 = submission.author\n",
    "    submission_add3 = submission.created_utc\n",
    "    submission_add4 = submission.is_self\n",
    "    submission_add5 = submission.name\n",
    "    submission_add6 = submission.selftext\n",
    "    submission_add7 = submission.title\n",
    "    submission_add8 = submission.url\n",
    "    submission_add9 = submission.subreddit.id\n",
    "    data_praw.append([submission_add, submission_add2,submission_add3, submission_add4, submission_add5, submission_add6, submission_add7, submission_add8, submission_add9])\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_praw_df = pd.DataFrame(data_praw, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id'])\n",
    "#data_praw_df.to_csv(r'submissions_praw_05302019.csv')\n",
    "#data_praw_df.to_json(r'submissions_praw_05292019.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_praw_df.to_pickle(r'submissions_praw_05302019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_praw_df2 = pd.read_pickle(r'submissions_praw_05302019.pkl')\n",
    "\n",
    "data_praw_df2 = data_praw_df2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########created is_russian, date_created and hour_created variables ############\n",
    "i = 0\n",
    "while i < len(data_praw_df2):\n",
    "    if data_praw_df2[i][1] in authors:\n",
    "        indicator = 1\n",
    "    elif data_praw_df2[i][1] not in authors:\n",
    "        indicator = 0\n",
    "    date_created = datetime.utcfromtimestamp(data_praw_df2[i][2]).strftime('%Y-%m-%d')\n",
    "    hour_created = datetime.utcfromtimestamp(data_praw_df2[i][2]).strftime('%H')\n",
    "    data_praw_df2[i].extend([indicator, date_created, hour_created])\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########make sure date is not after 2016##########\n",
    "data_praw_3 = [sublist for sublist in data_praw_df2 if sublist[2] < 1483142401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_praw_df_3 = pd.DataFrame(data_praw_3 , columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'date_created', 'hour_created'])\n",
    "data_praw_df_3.to_pickle(r'submission_praw2_05302019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Exploratory Data Analysis##########\n",
    "\n",
    "data_praw_df_31 = pd.read_pickle(r'submission_praw2_05302019.pkl')\n",
    "data_praw_df_eda = data_praw_df_31\n",
    "\n",
    "\n",
    "data_praw_df_eda['count'] = 1\n",
    "\n",
    "df = data_praw_df_eda[['is_russian', 'date_created', 'count']]\n",
    "df['Date'] = pd.to_datetime(df['date_created'])\n",
    "df = df.groupby(['is_russian', pd.Grouper(key='Date', freq='W-MON')])['count'].sum().reset_index().sort_values('Date')\n",
    "\n",
    "####Plot: number of posts per week #####\n",
    "\n",
    "df_date_russian = df[df.is_russian == 1]\n",
    "df_date_user = df[df.is_russian == 0]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot('Date', 'count', data=df_date_russian, marker='', linewidth=2)\n",
    "plt.plot('Date', 'count', data=df_date_user, marker='', linewidth=2)\n",
    "plt.legend(labels = ('Russian user', 'Normal user'), loc=1, prop={'size': 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = data_praw_df_eda[['is_russian', 'hour_created', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Plot: number of posts each hour of the day##########\n",
    "df_hour_groupby = df_hour.groupby(['is_russian', 'hour_created'])['count'].sum().reset_index().sort_values('hour_created')\n",
    "df_hour_russian = df_hour_groupby[df_hour_groupby.is_russian == 1]\n",
    "df_hour_user = df_hour_groupby[df_hour_groupby.is_russian == 0]\n",
    "\n",
    "\n",
    "line = pd.DataFrame({'is_russian': 1, 'hour_created': '04', 'count': '0'}, index=[4])\n",
    "df_hour_russian2 = pd.concat([df_hour_russian.iloc[:4], line, df_hour_russian.iloc[4:]]).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot('hour_created', 'count', data=df_hour_russian2, marker='', linewidth=2)\n",
    "plt.plot('hour_created', 'count', data=df_hour_user, marker='', linewidth=2)\n",
    "plt.legend(labels = ('Russian user', 'Normal user'), loc=2, prop={'size': 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('number of russian user posts:',len(data_praw_df_eda[data_praw_df_eda.is_russian == 1]))\n",
    "#print('number of normal user posts:', len(data_praw_df_eda[data_praw_df_eda.is_russian == 0]))\n",
    "#df_date_user.describe()\n",
    "#df_date_russian.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Train/Test set########\n",
    "### approx. 70/30 split: 8590/3682, aim to get between 70-75%\n",
    "#1. get list of authors\n",
    "#2. split list into normal and russian\n",
    "#3. randomly select either normal or russian\n",
    "#4. randomly select an author from the randomly chosen list\n",
    "#5. count number of subs in each set...when one reaches limit...put the rest in the other\n",
    "\n",
    "import random\n",
    "\n",
    "def unique(list1): \n",
    "    unique_list = [] \n",
    "    for x in list1: \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "all_users = [sub[1] for sub in data_praw_df_31]\n",
    "all_users_dist = unique(all_users)\n",
    "normal_user = [user for user in all_users_dist if not user in authors]\n",
    "russian_user = [user for user in all_users_dist if user in authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub = []\n",
    "length_train_sub = 0\n",
    "random.seed(30)\n",
    "\n",
    "while length_train_sub < 9204:\n",
    "    selected_set = random.sample(['normal_user', 'russian_user'],1)\n",
    "    if selected_set[0] == 'russian_user':\n",
    "        if len(russian_user) == 0:\n",
    "            selected_author = random.sample(normal_user,1)\n",
    "            normal_user.remove(selected_author[0])\n",
    "        elif len(russian_user) > 0:\n",
    "            selected_author = random.sample(russian_user,1)\n",
    "            russian_user.remove(selected_author[0])\n",
    "    elif selected_set[0] == 'normal_user':\n",
    "        if len(normal_user) == 0:\n",
    "            selected_author = random.sample(russian_user,1)\n",
    "            russian_user.remove(selected_author[0])\n",
    "        elif len(normal_user) > 0:\n",
    "            selected_author = random.sample(normal_user,1)\n",
    "            normal_user.remove(selected_author[0])\n",
    "    selected_sub = [sub for sub in data_praw_df_31 if sub[1] == selected_author[0]]\n",
    "    train_sub.extend(selected_sub)\n",
    "    length_train_sub = len(train_sub)\n",
    "    if length_train_sub > 8590:\n",
    "        length_train_sub += 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub_id = [sub[0] for sub in train_sub]\n",
    "test_sub = [sub for sub in data_praw_df_31 if not sub[0] in set(train_sub_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub_df = pd.DataFrame(train_sub, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'date_created'])\n",
    "test_sub_df = pd.DataFrame(test_sub, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'date_created'])\n",
    "\n",
    "train_sub_df.to_pickle(r'submissions_train_06022019.pkl')\n",
    "test_sub_df.to_pickle(r'submissions_test_06022019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Pre-processing Submission Title###########\n",
    "\n",
    "#Function to remove stopwords and punctuation, add POS tag and lemmatize)\n",
    "\n",
    "train_sub_df = pd.read_pickle(r'submissions_train_06032019.pkl')\n",
    "test_sub_df = pd.read_pickle(r'submissions_test_06032019.pkl')\n",
    "\n",
    "train_sub_df = train_sub_df.values.tolist()\n",
    "test_sub_df = test_sub_df.values.tolist()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def StopPunctTagLem(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    exclude_punct = set(string.punctuation)\n",
    "    add_set = {'...', '’', '``', '–', '”', \"''\", '‘', '“'}\n",
    "    word_list = [nltk.word_tokenize(sub[6]) for sub in dataset]\n",
    "\n",
    "    i = 0\n",
    "    filtered_sub_all = []\n",
    "    while i < len(word_list):\n",
    "        filtered_sub = [w.lower() for w in word_list[i]]\n",
    "        filtered_sub = [w.strip() for w in filtered_sub if not w in exclude_punct]\n",
    "        filtered_sub = [w for w in filtered_sub if not w in stop_words]\n",
    "        filtered_sub = [w for w in filtered_sub if not w in add_set]\n",
    "        filtered_sub_all.append(filtered_sub)\n",
    "        i += 1\n",
    "        \n",
    "    print('stop words and punctuation removed...')\n",
    "        \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    j = 0\n",
    "    filtered_sub_lem = []\n",
    "    while j < len(filtered_sub_all):\n",
    "        sub = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_sub_all[j]]\n",
    "        filtered_sub_lem.append(sub)\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print(str(j)+'/'+str(len(dataset)))\n",
    "    return filtered_sub_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Plot: top 20 words used by russian users#######\n",
    "train_sub_df = pd.read_pickle(r'submissions_train_06022019.pkl')\n",
    "test_sub_df = pd.read_pickle(r'submissions_test_06022019.pkl')\n",
    "\n",
    "train_sub_df = train_sub_df.values.tolist()\n",
    "test_sub_df = test_sub_df.values.tolist()\n",
    "\n",
    "df_train_and_test = train_sub_df+test_sub_df\n",
    "df_train_and_test = pd.DataFrame(df_train_and_test, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'date_created'])\n",
    "df_russian_text = df_train_and_test[df_train_and_test.is_russian == 1]\n",
    "df_user_text = df_train_and_test[df_train_and_test.is_russian == 0]\n",
    "\n",
    "df_russian_text = df_russian_text.values.tolist()\n",
    "df_user_text = df_user_text.values.tolist()\n",
    "\n",
    "russian_sub_preprocessed = StopPunctTagLem(df_russian_text)\n",
    "\n",
    "submission_long = [item for sublist in russian_sub_preprocessed for item in sublist]\n",
    "freq_sub_all = nltk.FreqDist(submission_long)\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.tick_params(labelsize = 20)\n",
    "freq_sub_all.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Plot: top 20 words used by normal users #######\n",
    "user_sub_preprocessed = StopPunctTagLem(df_user_text)\n",
    "submission_long = [item for sublist in user_sub_preprocessed for item in sublist]\n",
    "freq_sub_all = nltk.FreqDist(submission_long)\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.tick_params(labelsize = 20)\n",
    "freq_sub_all.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########getting pre-processed text#########\n",
    "train_sub_df = pd.read_pickle(r'submissions_train_06022019.pkl')\n",
    "test_sub_df = pd.read_pickle(r'submissions_test_06022019.pkl')\n",
    "\n",
    "train_sub_df = train_sub_df.values.tolist()\n",
    "test_sub_df = test_sub_df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Adding variables to create model########\n",
    "#adding date_created, hour_created variables, domain and russian_prop variable############\n",
    "import tldextract\n",
    "i = 0\n",
    "\n",
    "while i < len(train_sub_df):\n",
    "    extracted_domain = tldextract.extract(train_sub_df[i][7]).domain\n",
    "    if extracted_domain == 'donotshoot' or extracted_domain == 'blackmattersus':\n",
    "        russian_prop = 1\n",
    "    else:\n",
    "        russian_prop = 0\n",
    "    date_created = datetime.utcfromtimestamp(train_sub_df[i][2]).strftime('%Y-%m-%d')\n",
    "    hour_created = datetime.utcfromtimestamp(train_sub_df[i][2]).strftime('%H')\n",
    "    train_sub_df[i].extend([date_created, hour_created, extracted_domain, russian_prop])\n",
    "    i += 1\n",
    "    \n",
    "i = 0\n",
    "while i < len(test_sub_df):\n",
    "    extracted_domain = tldextract.extract(test_sub_df[i][7]).domain\n",
    "    if extracted_domain == 'donotshoot' or extracted_domain == 'blackmattersus':\n",
    "        russian_prop = 1\n",
    "    else:\n",
    "        russian_prop = 0\n",
    "    date_created = datetime.utcfromtimestamp(test_sub_df[i][2]).strftime('%Y-%m-%d')\n",
    "    hour_created = datetime.utcfromtimestamp(test_sub_df[i][2]).strftime('%H')\n",
    "    test_sub_df[i].extend([date_created, hour_created, extracted_domain, russian_prop])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub_df = pd.DataFrame(train_sub_df, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'datetime_created', 'date_created', 'hour_created', 'domain', 'russian_prop'])\n",
    "test_sub_df = pd.DataFrame(test_sub_df, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'datetime_created', 'date_created', 'hour_created','domain', 'russian_prop'])\n",
    "\n",
    "train_sub_df.to_pickle(r'submissions_train_06032019.pkl')\n",
    "test_sub_df.to_pickle(r'submissions_test_06032019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sub_df.to_csv(r'submissions_train_06032019.csv')\n",
    "#test_sub_df.to_csv(r'submissions_test_06032019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub_preprocessed = StopPunctTagLem(train_sub_df)\n",
    "test_sub_preprocessed = StopPunctTagLem(test_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####IGNORE:doc2vec#######\n",
    "#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#\n",
    "#sub_all_df = train_sub_preprocessed+test_sub_preprocessed\n",
    "#\n",
    "#tagged_subs = [TaggedDocument(words=subs, tags=[str(i)]) for i, subs in enumerate(sub_all_df)]\n",
    "#y_train = [sub[9] for sub in train_sub_df]\n",
    "#y_test = [sub[9] for sub in test_sub_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######IGNORE: doc2vec, model########\n",
    "#max_epochs = 50\n",
    "#vec_size = 50\n",
    "#alpha = 0.025\n",
    "#\n",
    "#model_sub = Doc2Vec(vector_size=vec_size,\n",
    "#                alpha=alpha, \n",
    "#                min_alpha=0.00025,\n",
    "#                min_count=1,\n",
    "#               dm =1)\n",
    "#  \n",
    "#model_sub.build_vocab(tagged_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for iteration in range(max_epochs):\n",
    "#    print('iteration {0}'.format(iteration))\n",
    "#    model_sub.train(train_tagged_subs,\n",
    "#                total_examples=model_sub.corpus_count,\n",
    "#                epochs=iteration)\n",
    "#    # decrease the learning rate\n",
    "#    model_sub.alpha -= 0.0002\n",
    "#    # fix the learning rate, no decay\n",
    "#    model_sub.min_alpha = model_sub.alpha\n",
    "#    \n",
    "#model_sub.save('./d2v_sub.model')\n",
    "#print('Model saved as d2v_sub.model')\n",
    "#model=Doc2Vec.load('d2v_sub.model')\n",
    "\n",
    "#sub_all_doc2vec=[]\n",
    "#i = 0\n",
    "#while i < len(sub_all_df):\n",
    "#    sub_doc2vec = model.infer_vector(sub_all_df[i])\n",
    "#    sub_all_doc2vec.append(sub_doc2vec)\n",
    "#    i +=1\n",
    "#print(sub_all_doc2vec)\n",
    "#pd.DataFrame(sub_all_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####IGNORE: doc2vec, PCA######\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components = 1)\n",
    "#pc_model= pca.fit(pd.DataFrame(sub_all_doc2vec))\n",
    "#principalComponents_train = pca.transform(pd.DataFrame(sub_all_doc2vec))\n",
    "#principalComponents_test = pca.transform(pd.DataFrame(test_sub_Tfidf.toarray()))\n",
    "#principalDf_train = pd.DataFrame(data = principalComponents_train,columns = ['principal component 1', 'principal component 2'])\n",
    "#principalDf_test = pd.DataFrame(data = principalComponents_test,columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "#per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "#labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "#plt.figure(figsize=(50, 25))\n",
    "#plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "#plt.ylabel('Percentage of Explained Variance')\n",
    "#plt.xlabel('Principal Component')\n",
    "#plt.title('Scree Plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####IGNORE: doc2vec, t-sne#####\n",
    "#from sklearn.manifold import TSNE\n",
    "#\n",
    "#X_tsne = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(sub_all_doc2vec)\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "#y_all = y_train+y_test\n",
    "#\n",
    "#plt.figure(figsize=(50, 25))\n",
    "#plt.subplot(121)\n",
    "#plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######TF-IDF########\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=50)\n",
    "#Tfidf_vect = TfidfVectorizer(max_features=50, ngram_range = (2,2))\n",
    "sub_all_df = train_sub_preprocessed+test_sub_preprocessed\n",
    "sub_all_df = [\" \".join(sub) for sub in sub_all_df]\n",
    "sub_all_df = pd.DataFrame(sub_all_df, columns = ['text'])\n",
    "\n",
    "train_sub_df2 = [\" \".join(sub) for sub in train_sub_preprocessed]\n",
    "train_sub_df2 = pd.DataFrame(train_sub_df2, columns = ['text'])\n",
    "\n",
    "test_sub_df2 = [\" \".join(sub) for sub in test_sub_preprocessed]\n",
    "test_sub_df2 = pd.DataFrame(test_sub_df2, columns = ['text'])\n",
    "\n",
    "Tfidf_vect.fit(sub_all_df['text'])\n",
    "train_sub_Tfidf = Tfidf_vect.transform(train_sub_df2['text'])\n",
    "test_sub_Tfidf = Tfidf_vect.transform(test_sub_df2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sub_Tfidf_2 = Tfidf_vect.fit_transform(train_sub_df2['text'])\n",
    "train_sub_Tfidf_df = pd.DataFrame(train_sub_Tfidf.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "test_sub_Tfidf_df = pd.DataFrame(test_sub_Tfidf.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "sub_all_Tfidf_df = train_sub_Tfidf_df+test_sub_Tfidf_df\n",
    "#train_sub_Tfidf_df.to_csv(r'/Users/janellashu/Desktop/train_bigram_06032019.csv')\n",
    "#test_sub_Tfidf_df.to_csv(r'/Users/janellashu/Desktop/test_bigram_06032019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########IGNORE:k-means##########\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#number_of_clusters = 2\n",
    "#km = KMeans(n_clusters=number_of_clusters)\n",
    "#km.fit(train_sub_Tfidf)\n",
    "#print(\"Top terms per cluster:\")\n",
    "#order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "#terms = Tfidf_vect.get_feature_names()\n",
    "#for i in range(number_of_clusters):\n",
    "#    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "#    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "#results = pd.DataFrame()\n",
    "#results['text'] = train_sub_df2['text']\n",
    "#results['category'] = km.labels_\n",
    "#results['y'] = y_train\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####PCA########\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "pc_model= pca.fit(pd.DataFrame(train_sub_Tfidf.toarray()))\n",
    "#principalComponents_test = pca.transform(pd.DataFrame(test_sub_Tfidf.toarray()))\n",
    "#principalDf_train = pd.DataFrame(data = principalComponents_train,columns = ['principal component 1', 'principal component 2'])\n",
    "#principalDf_test = pd.DataFrame(data = principalComponents_test,columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######PCA of unigram##########\n",
    "pca_2 = PCA(n_components = 3)\n",
    "pc_model2= pca_2.fit(pd.DataFrame(train_sub_Tfidf.toarray()))\n",
    "principalComponents_train = pca_2.transform(pd.DataFrame(train_sub_Tfidf.toarray()))\n",
    "principalComponents_test = pca_2.transform(pd.DataFrame(test_sub_Tfidf.toarray()))\n",
    "\n",
    "train_pca = pd.DataFrame(principalComponents_train)\n",
    "train_pca.columns = ['PC1', 'PC2', 'PC3']\n",
    "\n",
    "test_pca = pd.DataFrame(principalComponents_test)\n",
    "test_pca.columns = ['PC1', 'PC2', 'PC3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents_train_list = principalComponents_train.tolist()\n",
    "train_pca_other = []\n",
    "i=0\n",
    "\n",
    "while i < len(principalComponents_train_list):\n",
    "    train_pca_other_add = train_sub_df[i]\n",
    "    train_pca_other_add.extend(principalComponents_train_list[i])\n",
    "    train_pca_other.append(train_pca_other_add)\n",
    "    i += 1\n",
    "\n",
    "principalComponents_test_list = principalComponents_test.tolist()\n",
    "test_pca_other = []\n",
    "i=0\n",
    "\n",
    "while i < len(principalComponents_test_list):\n",
    "    test_pca_other_add = test_sub_df[i]\n",
    "    test_pca_other_add.extend(principalComponents_test_list[i])\n",
    "    test_pca_other.append(test_pca_other_add)\n",
    "    i += 1\n",
    "test_pca_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used bigram\n",
    "train_pca_other_df = pd.DataFrame(train_pca_other, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'datetime_created', 'date_created', 'hour_created', 'domain', 'russian_prop', 'PC1', 'PC2', 'PC3',])\n",
    "train_pca_other_df['black man'] = train_sub_Tfidf_df['black man']\n",
    "train_pca_other_df['chicago cop'] = train_sub_Tfidf_df['chicago cop']\n",
    "train_pca_other_df['kill unarmed'] = train_sub_Tfidf_df['kill unarmed']\n",
    "\n",
    "test_pca_other_df = pd.DataFrame(test_pca_other, columns = ['id', 'author','created_utc' ,'is_self', 'name', 'selftext','title', 'url', 'subreddit_id', 'is_russian', 'datetime_created', 'date_created', 'hour_created', 'domain', 'russian_prop', 'PC1', 'PC2', 'PC3'])\n",
    "test_pca_other_df['black man'] = test_sub_Tfidf_df['black man']\n",
    "test_pca_other_df['chicago cop'] = test_sub_Tfidf_df['chicago cop']\n",
    "test_pca_other_df['kill unarmed'] = test_sub_Tfidf_df['kill unarmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-run td-idf for single word and run this cell\n",
    "train_pca_other_df['cop'] = train_sub_Tfidf_df['cop']\n",
    "train_pca_other_df['black'] = train_sub_Tfidf_df['black']\n",
    "train_pca_other_df['man'] = train_sub_Tfidf_df['man']\n",
    "train_pca_other_df['teen'] = train_sub_Tfidf_df['teen']\n",
    "\n",
    "\n",
    "test_pca_other_df['cop'] = test_sub_Tfidf_df['cop']\n",
    "test_pca_other_df['black'] = test_sub_Tfidf_df['black']\n",
    "test_pca_other_df['man'] = test_sub_Tfidf_df['man']\n",
    "test_pca_other_df['teen'] = test_sub_Tfidf_df['teen']\n",
    "\n",
    "\n",
    "train_pca_other_df.to_csv(r'train_pca_other_df06032019.csv')\n",
    "test_pca_other_df.to_csv(r'test_pca_other_df06032019.csv')\n",
    "#Note: PC1, PC2, PC3 are from PCA of tf-idf of unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pca_other_df['black'] = train_sub_Tfidf_df['black']\n",
    "#train_pca_other_df['man'] = train_sub_Tfidf_df['man']\n",
    "#train_pca_other_df['teen'] = train_sub_Tfidf_df['teen']\n",
    "\n",
    "#test_pca_other_df['cop'] = test_sub_Tfidf_df['cop']\n",
    "#test_pca_other_df['black'] = test_sub_Tfidf_df['black']\n",
    "#test_pca_other_df['man'] = test_sub_Tfidf_df['man']\n",
    "#test_pca_other_df['teen'] = test_sub_Tfidf_df['teen']\n",
    "\n",
    "#train_pca_other_df.to_pickle(r'/Users/janellashu/Desktop/train_pca_other_df06032019.pkl')\n",
    "#test_pca_other_df.to_pickle(r'/Users/janellashu/Desktop/test_pca_other_d06032019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Logistic Regression Model#########\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_pca_other_df = pd.read_csv(r'/Users/janellashu/Desktop/418 project misc_FIX/train_pca_other_df06032019.csv')\n",
    "train_pca_other_df = train_pca_other_df.drop(columns = 'Unnamed: 0')\n",
    "test_pca_other_df = pd.read_csv(r'test_pca_other_df06032019.csv')\n",
    "test_pca_other_df = test_pca_other_df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "m=LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "#######changed different variables here to create different models\n",
    "m.fit(train_pca_other_df[['PC1','PC2']],train_pca_other_df.is_russian)\n",
    "#m.fit(train_pca_other_df[['cop','black', 'man', 'teen']],train_pca_other_df.is_russian)\n",
    "#m.fit(train_pca_other_df[['russian_prop','black man']],train_pca_other_df.is_russian)\n",
    "\n",
    "y_pred_prob = m.predict_proba(test_pca_other_df[['russian_prop','black man']]).tolist()\n",
    "y_pred_prob = [prob[1] for prob in y_pred_prob]\n",
    "y_pred_prob\n",
    "\n",
    "#threshold = 0.1 for first 2 models\n",
    "#threshold = 0.5 got 3rd model\n",
    "y_pred = []\n",
    "i = 0\n",
    "while i < len(y_pred_prob):\n",
    "    if y_pred_prob[i] > 0.5:\n",
    "        y_pred_add = '1'\n",
    "    else:\n",
    "        y_pred_add = '0'  \n",
    "    y_pred.extend(y_pred_add)\n",
    "    i +=1\n",
    "    \n",
    "cross_tab = pd.DataFrame(list(test_pca_other_df.is_russian), columns = ['actual'])\n",
    "cross_tab['predicted'] = y_pred\n",
    "cross_tab['actual'] = pd.Categorical(cross_tab['actual'])\n",
    "cross_tab['predicted'] = pd.Categorical(cross_tab['predicted'])\n",
    "my_tab = pd.crosstab(index=cross_tab['actual'], columns=cross_tab['predicted'])  \n",
    "my_tab\n",
    "\n",
    "m.coef_\n",
    "m.intercept_\n",
    "\n",
    "#TPR\n",
    "#115/221 = 0.52\n",
    "\n",
    "#Precision\n",
    "#115/129 = 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########IGNORE:Decision tree#########\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#train_pca_other_df.is_russian = train_pca_other_df.is_russian\n",
    "#train_pca_other_df.created_utc = train_pca_other_df.created_utc\n",
    "#train_pca_other_df.russian_prop = train_pca_other_df.russian_prop\n",
    "#X_train_tree = train_pca_other_df[['created_utc', 'russian_prop', 'black man']]\n",
    "#Y_train_tree = train_pca_other_df.is_russian\n",
    "\n",
    "#clf = DecisionTreeClassifier()\n",
    "#m2 = clf.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "#X_test_tree = test_pca_other_df[['created_utc', 'russian_prop', 'black man']]\n",
    "#Y_actual_tree = test_pca_other_df.is_russian\n",
    "\n",
    "#y_pred = clf.predict(X_test_tree)\n",
    "\n",
    "#cross_tab = pd.DataFrame(list(test_pca_other_df.is_russian), columns = ['actual'])\n",
    "#cross_tab['predicted'] = y_pred\n",
    "#cross_tab['actual'] = pd.Categorical(cross_tab['actual'])\n",
    "#cross_tab['predicted'] = pd.Categorical(cross_tab['predicted'])\n",
    "#my_tab = pd.crosstab(index=cross_tab['actual'], columns=cross_tab['predicted'])  \n",
    "#my_tab\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
