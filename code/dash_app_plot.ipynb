{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "#import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "train_sub_df = pd.read_pickle(r'submissions_train_06032019.pkl')\n",
    "test_sub_df = pd.read_pickle(r'submissions_test_06032019.pkl')\n",
    "\n",
    "sub_df = pd.concat([train_sub_df,test_sub_df])\n",
    "sub_df=sub_df.reset_index(drop=True)\n",
    "sub_df = sub_df.values.tolist()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def StopPunctTagLem(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    exclude_punct = set(string.punctuation)\n",
    "    add_set = {'...', '’', '``', '–', '”', \"''\", '‘', '“'}\n",
    "    word_list = [nltk.word_tokenize(sub[6]) for sub in dataset]\n",
    "\n",
    "    i = 0\n",
    "    filtered_sub_all = []\n",
    "    while i < len(word_list):\n",
    "        filtered_sub = [w.lower() for w in word_list[i]]\n",
    "        filtered_sub = [w.strip() for w in filtered_sub if not w in exclude_punct]\n",
    "        filtered_sub = [w for w in filtered_sub if not w in stop_words]\n",
    "        filtered_sub = [w for w in filtered_sub if not w in add_set]\n",
    "        filtered_sub_all.append(filtered_sub)\n",
    "        i += 1\n",
    "        \n",
    "    print('stop words and punctuation removed...')\n",
    "        \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    j = 0\n",
    "    filtered_sub_lem = []\n",
    "    while j < len(filtered_sub_all):\n",
    "        sub = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_sub_all[j]]\n",
    "        filtered_sub_lem.append(sub)\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print(str(j)+'/'+str(len(dataset)))\n",
    "    return filtered_sub_lem   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sub_preprocessed = StopPunctTagLem(train_sub_df)\n",
    "#test_sub_preprocessed = StopPunctTagLem(test_sub_df)\n",
    "sub_df_preprocessed = StopPunctTagLem(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Doc2vec#######\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_subs = [TaggedDocument(words=subs, tags=[str(i)]) for i, subs in enumerate(sub_df_preprocessed)]\n",
    "#y_train = [sub[9] for sub in train_sub_df]\n",
    "#y_test = [sub[9] for sub in test_sub_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "vec_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "model_sub = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "               dm =1)\n",
    "  \n",
    "model_sub.build_vocab(tagged_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(max_epochs):\n",
    "    print('iteration {0}'.format(iteration))\n",
    "    model_sub.train(tagged_subs,\n",
    "                total_examples=model_sub.corpus_count,\n",
    "                epochs=iteration)\n",
    "    # decrease the learning rate\n",
    "    model_sub.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model_sub.min_alpha = model_sub.alpha\n",
    "    \n",
    "model_sub.save('./d2v_sub.model')\n",
    "print('Model saved as d2v_sub.model')\n",
    "model=Doc2Vec.load('d2v_sub.model')\n",
    "\n",
    "sub_all_doc2vec=[]\n",
    "i = 0\n",
    "while i < len(sub_all_df):\n",
    "    sub_doc2vec = model.infer_vector(sub_all_df[i])\n",
    "    sub_all_doc2vec.append(sub_doc2vec)\n",
    "    i +=1\n",
    "#print(sub_all_doc2vec)\n",
    "#pd.DataFrame(sub_all_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####T-SNE#####\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_tsne = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(sub_all_doc2vec)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#y_all = [sub[9] for sub in sub_df]\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######IGNORE: plot of submission titles#######\n",
    "plt.figure(figsize=(50, 25))\n",
    "plt.subplot(121)\n",
    "y_all = [sub[9] for sub in sub_df]\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_sub= pd.DataFrame(X_tsne, columns = ['Dimension1', 'Dimension2'])\n",
    "scatter_sub['y'] = y_all\n",
    "scatter_sub['SubmissionTitle'] = [sub[6] for sub in sub_df]\n",
    "scatter_sub.to_csv(r'submissions_plot_06032019.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
